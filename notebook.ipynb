{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Leveling Difficulty in Candy Crush Saga\n",
    "\n",
    "Today's analysis will lead us to a world fulfilled with divine puzzle adventures at the side of Tiffi and Mr. Toffee, in which we'll glimpse the success rate of more than 6800 peers with eagerness for treats.\n",
    "\n",
    "The game is [Candy Crush Saga](https://www.king.com/game/candycrush) a record-breaking mobile game developed by King, a subsidiary of Activision Blizzard since 2016.\n",
    "\n",
    "In terms of its gameplay, this game used to have a total of 816 episodes, until one of the major add-ons was removed (Dreamworld). The point is that now the game has 771 episodes, in which the player has 5 episodes per world, and, each episode contains contain 15 levels approximately.\n",
    "\n",
    "If you are one of the few that haven't played Candy Crush, here's a short intro:\n",
    "\n",
    "<p align=center><a href=\"https://youtu.be/GaP5f0jVTWE\"><img src=\"https://static-prod.adweek.com/wp-content/uploads/2017/12/CandyCrushSagaHero.jpg\" style=\"width: 750px\"></a></p>\n",
    "\n",
    "## ‚ö†Ô∏è Introduction to problem\n",
    "\n",
    "### Hypothesis\n",
    "\n",
    "We'll review a game that potentially can lead any developer to many unseen problems, considering the abundance of levels. From the perspective of a customer, there can be several points of view that can emerge and, at the same time, can become unnoticed. That's why our diagnosis will start from 2 potential hypothesis\n",
    "\n",
    "1. The game is too easy so it became boring over time\n",
    "2. The game is too hard so the players leave it and become frustrated\n",
    "\n",
    "<p align=\"middle\"><img src=\"https://raw.githubusercontent.com/robguilarr/candy_crush_difficulty/master/plots/map.png\" style=\"width: 700px\"></p>\n",
    "\n",
    "### Potential Stakeholders\n",
    "\n",
    "None of the past hypotheses are the main intentions of the developers. So they require a Data Analyst to help with this task since the developers are seeing only the backend factors affecting the game, but it's also critical to consider those external ones that affect the experience for the player and the sustainability of this game for the company. Among the main stakeholders could be:\n",
    "\n",
    "- Level Designers: They work aligned with the rest of the Engineering Team because they still have a backend perspective and their next patch release needs to be aligned with the insights given by the analyst.\n",
    "- Mobile Designer & User Retention Expert: This is a game whose main income input is in-game purchases because it‚Äôs a [F2P](https://en.wikipedia.org/wiki/Free-to-play), the main source of income is centered in retain the engagement in the game and keeping the consumers on the platform.\n",
    "- Gameplay Engineer: They require to start working on the difficulty adjustment patch as soon as they receive the final statement.\n",
    "- Executive Producer: Besides Candy Crush being an IP with internal producers since it's developed and published by King, the [parent company](https://www.activisionblizzard.com/) will expect to have an [ROI](https://www.forbes.com/advisor/investing/roi-return-on-investment/) aligned with their expectations.\n",
    "- Players' community: They expect to have an endurable and great experience with a brief response in case of disconformities.\n",
    "\n",
    "\n",
    "**Note:** To facilitate the understanding of the roles of the development team, I invite you to take a look at **[this](https://miro.com/app/board/uXjVOwQVsRk=/?share_link_id=215280076044)** diagram that I designed.\n",
    "\n",
    "## üì• About the data\n",
    "\n",
    "### Collection process and structure\n",
    "\n",
    "Before start let‚Äôs import the libraries we‚Äôre going to need"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# For visualization\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "# Own layout design library\n",
    "from vizformatter.standards import layout_plotly\n",
    "\n",
    "# Load layout base objects\n",
    "sign, layout = layout_plotly(height= 720, width= 1000, font_size= 15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Due to the extensive size of possible episodes to analyze, we‚Äôll limit the analysis to just one of them, which exactly will have data available for 15 levels. To do this, the analysts need to request a sample from the telemetry systems to get data related to the number of attempts per player in each episode level\n",
    "\n",
    "Also, it‚Äôs important to mention that in terms of privacy, this analysis requires importing the data with the *player_id* codified for privacy reasons. Fortunately, in this case, Rasmus Baath, Data Science Lead at¬†[castle.io](https://castle.io/), provided us with a Dataset with a sample gathered in 2014"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"datasets/candy_crush.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that our data is **structured** and consists of 5 attributes:\n",
    "\n",
    "- *player_id*: a unique player id\n",
    "- *dt*: the date\n",
    "- *level*: the level number within the episode, from 1 to 15\n",
    "- *num_attempts*: number of level attempts for the player on that level and date\n",
    "- *num_success*: number of level attempts that resulted in a success/win for the player on that level and date\n",
    "\n",
    "## üîß Data Preprocessing\n",
    "\n",
    "Before starting the analysis we need to do some validations on the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count and display the number of unique players\n",
    "print(\"Number of players: \\n\", df.player_id.nunique(), '\\n',\n",
    "        \"Number of records: \\n\", len(df.player_id),'\\n')\n",
    "\n",
    "# Display the date range of the data\n",
    "print(\"Period for which we have data: \\nFrom: \",\n",
    "        min(df.dt), ' To:', max(df.dt))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Cleaning\n",
    "\n",
    "The data doesn‚Äôt require any kind of transformation and the data types are aligned with their purpose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Consistency\n",
    "\n",
    "The usability of the data it‚Äôs rather good, since we don‚Äôt count with ‚ÄúNAN‚Äù (Not A Number), ‚ÄúNA‚Äù (Not Available), or ‚ÄúNULL‚Äù (an empty set) values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function the plot the percentage of missing values\n",
    "def na_counter(df):\n",
    "    print(\"NaN Values per column:\")\n",
    "    print(\"\")\n",
    "    for i in df.columns:\n",
    "        percentage = 100 - ((len(df[i]) - df[i].isna().sum())/len(df[i]))*100\n",
    "\n",
    "        # Only return columns with more than 5% of NA values\n",
    "        if percentage > 5:\n",
    "            print(i+\" has \"+ str(round(percentage)) +\"% of Null Values\")\n",
    "        else:\n",
    "            continue\n",
    "\n",
    "# Execute function\n",
    "na_counter(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By this way, we can conclude that there were not errors in our telemetry logs during the data collection\n",
    "\n",
    "### Normalization\n",
    "\n",
    "Next, we can conclude there were no impossible numbers, except for a player that tried to complete the level 11 in 258 attempts with just 1 success. This is the only registry we exclude since it can be an influential outlier and we don‚Äôt rely on more attributes about him to create conclusions\n",
    "\n",
    "Noticing the distribution of the quartiles and comprehending the purpose of our analysis, we can validate that the data is comparable and doesn‚Äôt need transformations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[df.num_attempts != 258]\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîç Exploratory Analysis & In-game interpretations\n",
    "\n",
    "### Summary statistics\n",
    "\n",
    "Excluding the outliers we mentioned before, we got the next conclusions about their distribution and measurement:\n",
    "\n",
    "- *player_id*\n",
    "    - Interpretation: Not unique and counts with 6814 distinct values which make sense since there is a player with records of multiple levels\n",
    "    - Data type: Nominal\n",
    "    - Measurement type: Discrete/String\n",
    "- *dt*\n",
    "    - Interpretation: Only includes data from January 1st to January 7th of 2014. Also, the analysis won‚Äôt consider this as a lapse per player since the records per player are not continuous, so they will be limited as a timestamp\n",
    "    - Data type: Ordinal\n",
    "    - Measurement type: Temporal\n",
    "- *level*\n",
    "    - Interpretation: They're registered as numbers, but for further analysis will be transformed as factors. 50% of the records are equal to or less than level 9\n",
    "    - Data type: Ordinal\n",
    "    - Measurement type: Discrete/String\n",
    "- *num_attempts*\n",
    "    - Interpretation: The registries are consistent, the interquartile range mention that half of the players try between 1 and 7 time to complete each level. Furthermore, there are players with 0 attempts, so we need to evaluate if this is present at level 1, which can explain a problem in retention rate for that episode\n",
    "    - Data type: Numerical\n",
    "    - Measurement type: Integer\n",
    "- *num_success*\n",
    "    - Interpretation: Most of the players are casual gamers because 75% of them complete the level and don‚Äôt repeat it\n",
    "    - Data type: Numerical\n",
    "    - Measurement type: Integer\n",
    "\n",
    "### Levels played in Episode\n",
    "\n",
    "First, let‚Äôs examine the number of registries per player\n",
    "\n",
    "This will tell us, from the episode how many levels have each player recorded in the lapse of 7 days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from plotly.subplots import make_subplots\n",
    "\n",
    "# Group data of amount of levels recorded by player id\n",
    "countdf = df.groupby('player_id')['level'].nunique().reset_index()\n",
    "\n",
    "# Count the number amount of players according to amount of levels recorded by player\n",
    "countdf = countdf.groupby('level')['player_id'].nunique().reset_index()\n",
    "\n",
    "# Arrange data according to amount of levels\n",
    "countdf.level = [str(i)+'s' for i in countdf.level]\n",
    "countdf = countdf.sort_values('player_id', ascending= False)\n",
    "\n",
    "# Generate CumSum\n",
    "cumulative_per =  countdf.player_id / countdf.player_id.sum() * 100\n",
    "cumulative_per = cumulative_per.cumsum()\n",
    "\n",
    "# Format new DF\n",
    "countdf = pd.concat([countdf, cumulative_per], axis = 1)\n",
    "countdf.columns = [\"levels\",\"players\",\"Cum_per\"]\n",
    "\n",
    "# Pareto Chart\n",
    "linec = make_subplots(specs=[[{\"secondary_y\": True}]])\n",
    "\n",
    "# Bar plot graphic object\n",
    "linec.add_trace(go.Bar(x = countdf.levels, y = countdf.players, name = \"Players\", marker_color= \"#007FFF\"),\n",
    "                        secondary_y = False)\n",
    "\n",
    "# Scatter plot graphic object\n",
    "linec.add_trace(go.Scatter(x = countdf.levels, y = countdf.Cum_per/100, mode='lines+markers', name = \"Percentage\", marker_color= \"#FF5A5F\"),\n",
    "                        secondary_y = True)\n",
    "\n",
    "# Layout\n",
    "linec.update_layout(title = {'text':'Pareto Chart of Number of Levels recorded by player'},\n",
    "                    xaxis = {\"title\":\"Number of Levels recorded\"},\n",
    "                    yaxis = {\"title\":\"Unique players\"})\n",
    "linec.update_layout(layout)\n",
    "linec.update_yaxes(tickformat = \"0%\", title = \"Cumulative Percentage\", secondary_y = True)\n",
    "linec.update_layout(showlegend=False)\n",
    "linec.add_hline(y=0.8, line_dash = \"dash\", line_color=\"red\", opacity=0.5, secondary_y = True)\n",
    "linec.add_annotation(sign)\n",
    "linec.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the last Pareto chart, we can deduce that 80% of the 6814 players just count with 3 levels recorded of 15. But, since this was extracted from a random sample, this won‚Äôt affect our study\n",
    "\n",
    "### Difficulty of completing a level in a single try\n",
    "\n",
    "There is a combination of easier and challenging levels. Chance and skills make the number of attempts required to pass a level different from one player to another. The presumption is that difficult levels demand more tries on average than easier ones. That is, the harder a level is the lower the likelihood to pass that level in a single try\n",
    "\n",
    "In these circumstances, the¬†[Bernoulli process](https://en.wikipedia.org/wiki/Bernoulli_process) might be useful. As a *Boolean* result, there are only two possibilities, win or lose. This can be measured by a single parameter:\n",
    "\n",
    "$p_{win} = \\frac{\\Sigma wins }{\\Sigma attempts }$:  the probability of completing the level in a single attempt\n",
    "\n",
    "Let's calculate the difficulty $p_{win}$ individually for each of the 15 levels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "difficulty = df.groupby('level').agg(attempts = ('num_attempts', 'sum'),\n",
    "                                    wins =('num_success', 'sum')).reset_index()\n",
    "difficulty['p_win'] = difficulty.wins / difficulty.attempts\n",
    "difficulty"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have levels where 50% of players finished on the first attempt and others that are the opposite. But let‚Äôs visualize it through the episode, to make it clear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lineplot of Success Probability per Level\n",
    "line1 = px.line(difficulty, x='level', y=\"p_win\",\n",
    "                title = \"Probability of Level Success at first attempt\",\n",
    "                labels = {\"p_win\":\"Probability\", \"level\":\"Episode Level\"})\n",
    "line1.update_layout(layout)\n",
    "line1.update_xaxes(range=[1,15], tick0 = 1, dtick = 1)\n",
    "line1.update_yaxes(range=[0,0.7], tick0 = 0, dtick = 0.1)\n",
    "line1.update_layout(yaxis_tickformat = \"0%\")\n",
    "line1.add_annotation(sign)\n",
    "line1.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining hard levels\n",
    "\n",
    "It‚Äôs subjective what we can consider a hard level because not consistently depends on a single factor and for all player profile groups this can be different. So, for the outcomes of this study, we will arbitrarily assume that a difficult level is the one that has a probability to be completed in the first attempt of a 10% ($p_{win} < 10\\%$)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lineplot of Success Probability per Level\n",
    "line2 = go.Figure(go.Scatter(\n",
    "    x = difficulty.level,\n",
    "    y = difficulty.p_win))\n",
    "line2.update_layout(title = {'text':'Probability of Level Success at first attempt with Hard Levels'},\n",
    "                    xaxis = {\"title\":\"Episode Level\"},\n",
    "                    yaxis = {\"title\":\"Probability\"})\n",
    "line2.update_layout(layout)\n",
    "line2.update_xaxes(range=[1,15], tick0 = 1, dtick = 1)\n",
    "line2.update_layout(yaxis_tickformat = \"0%\")\n",
    "line2.update_layout(showlegend=False)\n",
    "line2.add_hrect(y0=0.02, y1=0.1, line_width=0, fillcolor=\"red\", opacity=0.2)\n",
    "line2.add_annotation(sign)\n",
    "line2.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From our predefined threshold, we see that the level digit is not aligned with its difficulty. While we have hard levels as 5, 8, 9, and 15; others like 13 and 15 are unleveraged and need to be rebalanced by the level designers\n",
    "\n",
    "### Measuring the uncertainty of success\n",
    "\n",
    "We should always report some calculation of the uncertainty of any provided numbers. Simply, because another sample will give us little different values for the difficulties measured by level.\n",
    "\n",
    "Here we will simply use the¬†[Standard error](https://en.wikipedia.org/wiki/Standard_error)¬†as a measure of uncertainty:\n",
    "\n",
    "$\\sigma_{error} \\approx \\frac{\\sigma_{sample}}{\\sqrt{n}}$\n",
    "\n",
    "Here n is the number of datapoints and $\\sigma_{sample}$ is the sample standard deviation. For a Bernoulli process, the sample standard deviation is:\n",
    "\n",
    "$\\sigma_{sample} = \\sqrt{p_{win}(1-p_{win})}$\n",
    "\n",
    "Therefore, we can calculate the standard error like this:\n",
    "\n",
    "$\\sigma_{error} \\approx \\sqrt{\\frac{p_{win}(1-p_{win})}{n} }$\n",
    "\n",
    "Consider that every level has been played *n* number of times and we have their difficulty $p_{win}$. Now, let's calculate the standard error for each level of this episode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Computing the standard error of p_win for each level\n",
    "difficulty['error'] = np.sqrt(difficulty.p_win * (1 - difficulty.p_win) / difficulty.attempts)\n",
    "difficulty"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have a measure of uncertainty for each levels' difficulty. As always, this would be more appealing if we plot it.\n",
    "\n",
    "Let's use¬†*error bars*¬†to show this uncertainty in the plot. We will set the height of the error bars to one standard error. The upper limit and the lower limit of each error bar should be defined by:\n",
    "\n",
    "$p_{win}  \\pm \\sigma_{error}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lineplot of Success Probability per Level\n",
    "line3 = px.line(difficulty, x='level', y=\"p_win\",\n",
    "                title = \"Probability of Level Success at first attempt with Error Bars\",\n",
    "                labels = {\"p_win\":\"Probability\", \"level\":\"Episode Level\"},\n",
    "                error_y=\"error\")\n",
    "line3.update_layout(layout)\n",
    "line3.update_xaxes(range=[0,16], tick0 = 1, dtick = 1)\n",
    "line3.update_yaxes(range=[0,0.65], tick0 = 0, dtick = 0.1)\n",
    "line3.update_layout(yaxis_tickformat = \"0%\")\n",
    "line3.add_hrect(y0=0.02, y1=0.1, line_width=0, fillcolor=\"red\", opacity=0.2)\n",
    "line3.add_annotation(sign)\n",
    "line3.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks like the difficulty estimates a very exact. Furthermore, for the hardest levels, the measure is even more precise, and that‚Äôs a good point because from this we can make valid conclusions based on that levels\n",
    "\n",
    "As a curious fact, also we can measure the probability of completing all the levels of that episode in a single attempt, just for fun"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The probability of completing the episode without losing a single time\n",
    "prob = 1\n",
    "\n",
    "for i in difficulty.p_win:\n",
    "    prob = prob*i\n",
    "\n",
    "# Printing it out\n",
    "print(\"Probability of Success in one single attempt \\nfor whole episode: \", prob*100, \"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üóíÔ∏è Final thoughts & takeaways\n",
    "\n",
    "**What can the stakeholders understand and take into consideration?**\n",
    "\n",
    "From the sample extracted we conclude that just 33% of the levels are considered of high difficulty, which it‚Äôs acceptable since each episode counts with 15 levels, so by now the level designer should not worry about leveling the difficulty\n",
    "\n",
    "**What could the stakeholders do to take action ?**\n",
    "\n",
    "As a suggestion, in the case that the Publisher decides to invest more in in-game mechanics, a solution for a long-time and reactive engagement could be the use of Machine Learning to generate a [DGDB](https://en.wikipedia.org/wiki/Dynamic_game_difficulty_balancing) as some competitors have adapted in IPs like EA Sports FIFA, Madden NFL or the ‚ÄúAI Director‚Äù of Left 4 Dead\n",
    "\n",
    "**What can stakeholders keep working on?** \n",
    "\n",
    "The way their level difficulty design work today is precise since our first hypothesis was that the game wasn‚Äôt too linear to unengaged the player and churn as consequence. Because as we saw, the game has drastic variations in the levels 5-6 and 8-10, which can help to avoid frustrations in players\n",
    "\n",
    "---\n",
    "\n",
    "## ‚ÑπÔ∏è Additional Information\n",
    "\n",
    "- **About the article**\n",
    "\n",
    "Based on the dataset provided, we will not proceed with a retention analysis as mentioned above. Because the data is from a random episode, if this were episode one, this type of analysis could be useful as it can explain the pool of players who log in, created an account, or install the game but never start playing, causing traction problems. Therefore, this will be left as a limitation to the scope of the analysis.\n",
    "\n",
    "With acknowledgment to Rasmus Baraath for guiding this project. Which was developed for sharing knowledge while using cited sources of the material used.\n",
    "\n",
    "Thanks to you for reading as well.\n",
    "\n",
    "- **Related Content**\n",
    "\n",
    "‚Äî Rasmus Baath personal¬†[blog](https://www.sumsar.net/)\n",
    "\n",
    "‚Äî Anders Drachen personal¬†[website](https://andersdrachen.com/)\n",
    "\n",
    "- **Datasets**\n",
    "\n",
    "This project was developed with a dataset provided by Rasmus Baraath, which also can be downloaded at my <a href=\"https://github.com/robguilarr/candy_crush_difficulty/tree/master/datasets\">Github</a> repository."
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "740329b21a1252e1c253d583311b6d3f7744168cabc7e2151c4ba7a007513518"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
